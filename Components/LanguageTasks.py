# Components/LanguageTasks.py
"""
=============================================================================
IDENTIFICA√á√ÉO DE HIGHLIGHTS COM GPT - VERS√ÉO ATUAL
=============================================================================

‚ö†Ô∏è PROBLEMAS CONHECIDOS:
1. GPT retorna clips muito curtos (2-5s) mesmo com instru√ß√µes expl√≠citas
2. Prompt n√£o √© consistente - √†s vezes funciona, √†s vezes n√£o
3. Limite de tokens (120k) impede an√°lise de lives muito longas (5h+)
4. GPT n√£o entende bem o conceito de "contexto completo"
5. Temperature 0.3 pode estar tornando respostas muito mec√¢nicas

‚úÖ O QUE FUNCIONA:
- Detec√ß√£o de marcadores [RISO] na transcri√ß√£o
- Expans√£o for√ßada ao redor do cl√≠max
- Fallback para formato antigo (setup_start/reaction_end)

üîß MELHORIAS NECESS√ÅRIAS:
1. Usar few-shot learning (exemplos reais no prompt)
2. Dividir live em chunks menores (processar 30min por vez)
3. Implementar valida√ß√£o de dura√ß√£o ANTES de retornar
4. Adicionar l√≥gica de retry se clips forem muito curtos
5. Considerar usar modelo diferente (GPT-4 ou Claude)
6. Criar prompt mais simples: "encontre timestamps de momentos engra√ßados"
   e deixar a expans√£o 100% pro Python

üéØ SOLU√á√ÉO IDEAL FUTURA:
- Analisar √°udio diretamente (picos de volume = risadas)
- Usar ML para detectar padr√µes de risada no waveform
- Combinar an√°lise de √°udio + transcri√ß√£o
- Sistema de scoring baseado em m√∫ltiplos fatores

=============================================================================
"""

import os
import json
import re
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

if not os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API"):
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API")
if not os.getenv("OPENAI_API_KEY"):
    print("‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada no .env")


def _clean_llm_json(raw: str) -> str:
    """
    Remove markdown ```json do retorno do LLM.
    
    PROBLEMA: GPT √†s vezes retorna com ```json e √†s vezes sem
    SOLU√á√ÉO: Regex para limpar ambos os casos
    """
    raw = raw.strip()
    if raw.startswith("```"):
        raw = re.sub(r"^```(?:json)?", "", raw, flags=re.IGNORECASE).strip()
        raw = re.sub(r"```$", "", raw).strip()
    return raw


def GetHighlights(transcript_text: str, video_duration_min: float = 240):
    """
    Retorna highlights com DURA√á√ÉO FOR√áADA de 45-180s.
    
    FLUXO ATUAL:
    1. Envia prompt pro GPT pedindo "cl√≠max" de momentos
    2. GPT retorna lista de timestamps
    3. Python expande ao redor do cl√≠max (40% antes, 60% depois)
    
    ‚ö†Ô∏è PROBLEMA: GPT √†s vezes retorna mesmo timestamp repetido (climax: 44.0, 44.0, 44.0...)
    ‚ö†Ô∏è PROBLEMA: N√£o h√° garantia que o "cl√≠max" est√° correto
    
    üîß MELHORIA NECESS√ÅRIA:
    - Validar que timestamps s√£o diferentes
    - Adicionar margem de erro (se muitos clips no mesmo segundo, distribuir)
    - Implementar retry l√≥gico se resposta for ruim
    """
    llm = ChatOpenAI(
        model="gpt-4o-mini",  # CONSIDERA√á√ÉO: Testar gpt-4-turbo-preview
        temperature=0.3  # CONSIDERA√á√ÉO: Testar 0.5-0.7 para mais varia√ß√£o
    )

    # C√ÅLCULO DE MOMENTOS: 5h = 300min ‚Üí 300/4 = 75 momentos
    # PROBLEMA: Pedir muitos momentos pode resultar em qualidade baixa
    # MELHORIA: Pedir menos (20-30) mas com crit√©rio mais rigoroso
    num_moments = min(60, max(30, int(video_duration_min / 4)))

    prompt = ChatPromptTemplate.from_template(
        """
Voc√™ √© editor de Shorts VIRAIS de GAMES.

ENCONTRE momentos com RISADAS, FAILS, CLUTCHES, RAGE.

Procure na transcri√ß√£o por:
- "KKKK", "hahaha", "[RISO]"  # <- Marcadores adicionados pelo Transcription.py
- "caralho!", "porra!", "mano!"
- "N√ÉOOO!", "CONSEGUI!", "WTF"

Para cada momento, retorne o timestamp do CL√çMAX.

JSON:
[
  {{"climax": 380.5, "reason": "rizada + fail"}},
  {{"climax": 1250.0, "reason": "clutch √©pico"}}
]

Retorne {num_moments} momentos.

TRANSCRI√á√ÉO:
{transcript}
"""
    )
    # PROBLEMA DO PROMPT:
    # 1. Muito gen√©rico - n√£o d√° exemplos concretos
    # 2. N√£o explica o que √© "cl√≠max" claramente
    # 3. N√£o penaliza repeti√ß√µes
    # 
    # MELHORIA SUGERIDA:
    # - Adicionar 3-5 exemplos reais (few-shot)
    # - Especificar: "climax = momento exato da risada/rea√ß√£o"
    # - Adicionar: "NUNCA repita o mesmo timestamp"

    chain = prompt | llm
    response = chain.invoke({
        "transcript": transcript_text[:120000],  # LIMITA√á√ÉO: 120k tokens = ~2-3h de live
        "num_moments": num_moments
    })
    # PROBLEMA: Live de 5h n√£o cabe em 120k tokens
    # SOLU√á√ÉO FUTURA: Dividir em chunks de 30min, processar separadamente

    raw = response.content
    print(f"üîç DEBUG - Resposta do GPT (primeiros 500 chars):")
    print(raw[:500])
    # MANTER ESSE DEBUG - √∫til para diagnosticar problemas
    
    cleaned = _clean_llm_json(raw)

    try:
        moments = json.loads(cleaned)
    except Exception as e:
        print(f"‚ùå Erro ao parsear JSON: {e}")
        print(f"Raw: {raw[:500]}")
        return []
        # MELHORIA: Implementar retry com prompt simplificado se JSON falhar

    # P√ìS-PROCESSAMENTO: FOR√áAR DURA√á√ïES CORRETAS
    valid_clips = []
    seen_timestamps = set()  # ADICIONAR: Evitar duplicatas
    
    for m in moments:
        try:
            # Pegar o cl√≠max (ou usar setup_start se for o formato antigo)
            if "climax" in m:
                climax = float(m["climax"])
            elif "punchline" in m:
                climax = float(m["punchline"])
            else:
                continue
            
            # VERIFICAR DUPLICATAS
            # PROBLEMA: GPT √†s vezes retorna climax: 44.0 repetido 50x
            if climax in seen_timestamps:
                continue  # Pular duplicatas
            seen_timestamps.add(climax)
            
            # FOR√áAR dura√ß√£o 45-180s ao redor do cl√≠max
            duration = 60  # Dura√ß√£o padr√£o
            
            # AJUSTE DIN√ÇMICO baseado no motivo
            # CONSIDERA√á√ÉO: Isso pode n√£o ser ideal - testar dura√ß√µes fixas
            reason = m.get("reason", "").lower()
            if "clutch" in reason or "√©pico" in reason:
                duration = 90  # Clutches precisam de mais contexto
            elif "riso" in reason or "kkkk" in reason:
                duration = 60  # Risadas s√£o mais curtas
            elif "fail" in reason:
                duration = 75
            
            # Calcular start e end ao redor do cl√≠max
            # 40% antes (setup) + 60% depois (rea√ß√£o)
            start = max(0, climax - (duration * 0.4))
            end = climax + (duration * 0.6)
            
            # VALIDA√á√ÉO ADICIONAL NECESS√ÅRIA:
            # - Verificar se n√£o ultrapassa dura√ß√£o do v√≠deo
            # - Garantir que start < end (√≥bvio mas importante)
            # - Verificar se h√° palavras suficientes nesse intervalo
            
            valid_clips.append({
                "start": start,
                "end": end,
                "reason": m.get("reason", ""),
                "score": 1.0  # FUTURO: Calcular score real baseado em m√∫ltiplos fatores
            })
            
        except (KeyError, ValueError, TypeError) as e:
            continue

    print(f"‚úÖ {len(valid_clips)} highlights encontrados (expans√£o for√ßada aplicada)")
    
    # ORDENAR POR TIMESTAMP para facilitar deduplica√ß√£o posterior
    valid_clips = sorted(valid_clips, key=lambda x: x["start"])
    
    return valid_clips

# PR√ìXIMOS PASSOS SUGERIDOS:
# 1. Implementar an√°lise de √°udio direto (sem GPT)
# 2. Criar sistema de scoring por m√∫ltiplos fatores
# 3. Adicionar cache de resultados (n√£o processar mesma live 2x)
# 4. Implementar sistema de feedback (aprender com shorts aprovados)
# 5. Considerar usar modelo local (Whisper local + an√°lise de √°udio)
